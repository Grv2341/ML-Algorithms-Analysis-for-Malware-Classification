#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import xlrd
import matplotlib.pyplot as plt
algo=['Decision Tree','Naive Bayes','SGD','K-Nearest','Random Forest','SVM','Logistic Regression']
algo_time=[]
algo_acc=[]
algo_FP=[]
algo_FN=[]
workbook = xlrd.open_workbook('dataset_malwares.xls')
df=pd.read_excel('dataset_malwares.xls')
df.head()


# In[2]:


df.describe()


# In[3]:


X=df.iloc[:,0:20].values
y=np.array(df['Malware'])
X


# In[4]:


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import time
from sklearn.metrics import confusion_matrix
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
index=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]
t1=[]
for i in range(0,20):
    classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
    start=time.time()
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    end=time.time()
    cm = confusion_matrix(y_test, y_pred)
    t1.append(end-start);
plt.plot(index, t1, color='blue')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('Decision Tree (Time)')
plt.show()
algo_FN.append(cm[0][1])
algo_FP.append(cm[1][0])
cm


# In[5]:


algo_time.append(sum(t1)/len(t1))
end-start


# In[6]:


from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)
algo_acc.append(acc)
acc


# In[7]:


# Naive Bayes
from sklearn.naive_bayes import GaussianNB
t2=[]
for i in range(0,20):
    start=time.time()
    nb=GaussianNB()
    nb.fit(X_train,y_train)
    y_pred=nb.predict(X_test)
    end=time.time()
    cm = confusion_matrix(y_test, y_pred)
    t2.append(end-start);
plt.plot(index, t2, color='blue')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('Naive Bayes (Time)')
plt.show()
algo_FN.append(cm[0][1])
algo_FP.append(cm[1][0])
cm


# In[8]:


acc=accuracy_score(y_pred,y_test)
algo_acc.append(acc)
acc


# In[9]:


algo_time.append(sum(t2)/len(t2))
end-start


# In[10]:


#Stochastic Gradient Descent
from sklearn.linear_model import SGDClassifier
t3=[]
for i in range(0,20):
    start=time.time()
    sgd=SGDClassifier(loss='modified_huber',shuffle=True,random_state=101)
    sgd.fit(X_train,y_train)
    y_pred=nb.predict(X_test)
    end=time.time()
    t3.append(end-start);
plt.plot(index, t3, color='blue')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('SGD (Time)')
plt.show()
cm = confusion_matrix(y_test, y_pred)
algo_FN.append(cm[0][1])
algo_FP.append(cm[1][0])
cm


# In[11]:


acc=accuracy_score(y_pred,y_test)
algo_acc.append(acc)
acc


# In[12]:


algo_time.append(sum(t3)/len(t3))
end-start


# In[13]:


#K-nearest neighbour
from sklearn.neighbors import KNeighborsClassifier
t4=[]
for i in range(0,20):
    knn=KNeighborsClassifier(n_neighbors=15)
    start=time.time()
    knn.fit(X_train,y_train)
    y_pred=knn.predict(X_test)
    end=time.time()
    t4.append(end-start);
plt.plot(index, t4, color='blue')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('K-nearest neighbour (Time)')
plt.show()
cm = confusion_matrix(y_test, y_pred)
algo_FN.append(cm[0][1])
algo_FP.append(cm[1][0])
cm


# In[14]:


acc=accuracy_score(y_pred,y_test)
algo_acc.append(acc)
acc


# In[15]:


algo_time.append(sum(t4)/20)
end-start


# In[16]:


#Random Forest
from sklearn.ensemble import RandomForestClassifier
t5=[]
for i in range(0,20):
    rf = RandomForestClassifier(n_estimators=10, random_state = 0)
    start=time.time()
    rf.fit(X_train,y_train)
    y_pred = rf.predict(X_test)
    end=time.time()
    t5.append(end-start);
plt.plot(index, t5, color='blue')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('Random Forest (Time)')
plt.show()
cm = confusion_matrix(y_test, y_pred)
algo_FN.append(cm[0][1])
algo_FP.append(cm[1][0])
cm


# In[17]:


acc=accuracy_score(y_pred,y_test)
algo_acc.append(acc)
acc


# In[18]:


algo_time.append(sum(t5)/20)
end-start


# In[19]:


#Support Vector Machine
from sklearn.svm import SVC
t6=[]
for i in range(0,20):
    svm=SVC(kernel='linear',C=0.025,random_state=101)
    start=time.time()
    svm.fit(X_train,y_train)
    y_pred = rf.predict(X_test)
    end=time.time()
    t6.append(end-start);
plt.plot(index, t6, color='blue')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('SVM (Time)')
plt.show()
cm = confusion_matrix(y_test, y_pred)
algo_FN.append(cm[0][1])
algo_FP.append(cm[1][0])
cm


# In[20]:


acc=accuracy_score(y_pred,y_test)
algo_acc.append(acc)
acc


# In[21]:


algo_time.append(sum(t6)/20)
end-start


# In[22]:


#Logistic Regression
from sklearn.linear_model import LogisticRegression
t7=[]
for i in range(0,20):
    classifier = LogisticRegression(random_state = 101)
    start=time.time()
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    end=time.time()
    t7.append(end-start);
plt.plot(index, t7, color='blue')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('Logistic Regression (Time)')
plt.show()
cm = confusion_matrix(y_test, y_pred)
algo_FN.append(cm[0][1])
algo_FP.append(cm[1][0])
cm


# In[23]:


acc=accuracy_score(y_pred,y_test)
algo_acc.append(acc)
acc


# In[24]:


algo_time.append(sum(t7)/20)
end-start


# In[25]:


print(algo)
print(algo_time)
print(algo_acc)
print(algo_FP)
print(algo_FN)


# In[26]:


import matplotlib.pyplot as plt
index = np.arange(len(algo))
plt.bar(index, algo_time)
plt.xlabel('Algorithm', fontsize=10)
plt.ylabel('Time Taken', fontsize=10)
plt.xticks(index, algo, fontsize=10, rotation=60)
plt.title('Time Taken in Execution of each Algorithm')
plt.show()


# In[27]:


plt.bar(index, algo_acc)
plt.xlabel('Algorithm', fontsize=10)
plt.ylabel('Accuracy', fontsize=10)
plt.xticks(index, algo, fontsize=10, rotation=60)
plt.title('Accuracy of each Algorithm')
plt.show()


# In[28]:


plt.bar(index, algo_FP)
plt.xlabel('Algorithm', fontsize=10)
plt.ylabel('False Positives', fontsize=10)
plt.xticks(index, algo, fontsize=10, rotation=60)
plt.title('Number of Flase Positives for each Algorithm')
plt.show()


# In[29]:


plt.bar(index, algo_FN)
plt.xlabel('Algorithm', fontsize=10)
plt.ylabel('True Negative', fontsize=10)
plt.xticks(index, algo, fontsize=10, rotation=60)
plt.title('Number of True Negatives for each Algorithm')
plt.show()


# In[30]:


index=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]
plt.plot(index, t1, color='g',label='Decision Tree')
plt.plot(index, t2, color='red',label='Naive Bayes')
plt.plot(index, t3, color='blue',label='SGD')
plt.plot(index, t4, color='black',label='K-Nearest')
plt.plot(index, t5, color='c',label='Random Forest')
plt.plot(index, t7, color='y',label='Logistic Regression')
plt.xlabel('Iteration Number')
plt.ylabel('Time')
plt.title('Decision Tree (Time)')
plt.legend()
plt.show()


# In[ ]:




